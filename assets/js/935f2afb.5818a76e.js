"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[53],{1109:e=>{e.exports=JSON.parse('{"pluginId":"default","version":"current","label":"Next","banner":"unreleased","badge":true,"noIndex":false,"className":"docs-version-current","isLast":false,"docsSidebars":{"docs":[{"type":"link","label":"Quick Start","href":"/crawlee-cn/docs/next/quick-start/","docId":"quick-start/quick-start","unlisted":false},{"type":"category","label":"Introduction","collapsed":false,"items":[{"type":"link","label":"Setting up","href":"/crawlee-cn/docs/next/introduction/setting-up","docId":"introduction/setting-up","unlisted":false},{"type":"link","label":"First crawler","href":"/crawlee-cn/docs/next/introduction/first-crawler","docId":"introduction/first-crawler","unlisted":false},{"type":"link","label":"Adding more URLs","href":"/crawlee-cn/docs/next/introduction/adding-urls","docId":"introduction/adding-urls","unlisted":false},{"type":"link","label":"Real-world project","href":"/crawlee-cn/docs/next/introduction/real-world-project","docId":"introduction/real-world-project","unlisted":false},{"type":"link","label":"Crawling","href":"/crawlee-cn/docs/next/introduction/crawling","docId":"introduction/crawling","unlisted":false},{"type":"link","label":"Scraping","href":"/crawlee-cn/docs/next/introduction/scraping","docId":"introduction/scraping","unlisted":false},{"type":"link","label":"Saving data","href":"/crawlee-cn/docs/next/introduction/saving-data","docId":"introduction/saving-data","unlisted":false},{"type":"link","label":"Refactoring","href":"/crawlee-cn/docs/next/introduction/refactoring","docId":"introduction/refactoring","unlisted":false},{"type":"link","label":"Running in the Cloud","href":"/crawlee-cn/docs/next/introduction/deployment","docId":"introduction/deployment","unlisted":false}],"collapsible":true,"href":"/crawlee-cn/docs/next/introduction/"},{"type":"category","label":"Guides","items":[{"type":"link","label":"Request Storage","href":"/crawlee-cn/docs/next/guides/request-storage","docId":"guides/request-storage","unlisted":false},{"type":"link","label":"Result Storage","href":"/crawlee-cn/docs/next/guides/result-storage","docId":"guides/result-storage","unlisted":false},{"type":"link","label":"Configuration","href":"/crawlee-cn/docs/next/guides/configuration","docId":"guides/configuration","unlisted":false},{"type":"link","label":"CheerioCrawler","href":"/crawlee-cn/docs/next/guides/cheerio-crawler-guide","docId":"guides/cheerio-crawler-guide","unlisted":false},{"type":"link","label":"JavaScript rendering","href":"/crawlee-cn/docs/next/guides/javascript-rendering","docId":"guides/javascript-rendering","unlisted":false},{"type":"link","label":"Proxy Management","href":"/crawlee-cn/docs/next/guides/proxy-management","docId":"guides/proxy-management","unlisted":false},{"type":"link","label":"Session Management","href":"/crawlee-cn/docs/next/guides/session-management","docId":"guides/session-management","unlisted":false},{"type":"link","label":"Scaling our crawlers","href":"/crawlee-cn/docs/next/guides/scaling-crawlers","docId":"guides/scaling-crawlers","unlisted":false},{"type":"link","label":"Avoid getting blocked","href":"/crawlee-cn/docs/next/guides/avoid-blocking","docId":"guides/avoid-blocking","unlisted":false},{"type":"link","label":"JSDOMCrawler","href":"/crawlee-cn/docs/next/guides/jsdom-crawler-guide","docId":"guides/jsdom-crawler-guide","unlisted":false},{"type":"link","label":"Got Scraping","href":"/crawlee-cn/docs/next/guides/got-scraping","docId":"guides/got-scraping","unlisted":false},{"type":"link","label":"TypeScript Projects","href":"/crawlee-cn/docs/next/guides/typescript-project","docId":"guides/typescript-project","unlisted":false},{"type":"link","label":"Running in Docker","href":"/crawlee-cn/docs/next/guides/docker-images","docId":"guides/docker-images","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/crawlee-cn/docs/next/guides"},{"type":"category","label":"Deployment","items":[{"type":"link","label":"Deploy on Apify","href":"/crawlee-cn/docs/next/deployment/apify-platform","docId":"deployment/apify-platform","unlisted":false},{"type":"category","label":"Deploy on AWS","items":[{"type":"link","label":"Cheerio on AWS Lambda","href":"/crawlee-cn/docs/next/deployment/aws-cheerio","docId":"deployment/aws-cheerio","unlisted":false},{"type":"link","label":"Browsers on AWS Lambda","href":"/crawlee-cn/docs/next/deployment/aws-browsers","docId":"deployment/aws-browsers","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Deploy to Google Cloud","items":[{"type":"link","label":"Cheerio on GCP Cloud Functions","href":"/crawlee-cn/docs/next/deployment/gcp-cheerio","docId":"deployment/gcp-cheerio","unlisted":false},{"type":"link","label":"Browsers in GCP Cloud Run","href":"/crawlee-cn/docs/next/deployment/gcp-browsers","docId":"deployment/gcp-browsers","unlisted":false}],"collapsed":true,"collapsible":true}],"collapsed":true,"collapsible":true,"href":"/crawlee-cn/docs/next/deployment"},{"type":"category","label":"Examples","items":[{"type":"link","label":"Accept user input","href":"/crawlee-cn/docs/next/examples/accept-user-input","docId":"examples/accept-user-input","unlisted":false},{"type":"link","label":"Add data to dataset","href":"/crawlee-cn/docs/next/examples/add-data-to-dataset","docId":"examples/add-data-to-dataset","unlisted":false},{"type":"link","label":"Basic crawler","href":"/crawlee-cn/docs/next/examples/basic-crawler","docId":"examples/basic-crawler","unlisted":false},{"type":"link","label":"Cheerio crawler","href":"/crawlee-cn/docs/next/examples/cheerio-crawler","docId":"examples/cheerio-crawler","unlisted":false},{"type":"link","label":"Crawl all links on a website","href":"/crawlee-cn/docs/next/examples/crawl-all-links","docId":"examples/crawl-all-links","unlisted":false},{"type":"link","label":"Crawl multiple URLs","href":"/crawlee-cn/docs/next/examples/crawl-multiple-urls","docId":"examples/crawl-multiple-urls","unlisted":false},{"type":"link","label":"Crawl a website with relative links","href":"/crawlee-cn/docs/next/examples/crawl-relative-links","docId":"examples/crawl-relative-links","unlisted":false},{"type":"link","label":"Crawl a single URL","href":"/crawlee-cn/docs/next/examples/crawl-single-url","docId":"examples/crawl-single-url","unlisted":false},{"type":"link","label":"Crawl a sitemap","href":"/crawlee-cn/docs/next/examples/crawl-sitemap","docId":"examples/crawl-sitemap","unlisted":false},{"type":"link","label":"Crawl some links on a website","href":"/crawlee-cn/docs/next/examples/crawl-some-links","docId":"examples/crawl-some-links","unlisted":false},{"type":"link","label":"Using puppeteer-extra and playwright-extra","href":"/crawlee-cn/docs/next/examples/crawler-plugins/","docId":"examples/crawler-plugins/playwright-puppeteer-extra","unlisted":false},{"type":"link","label":"Export entire dataset to one file","href":"/crawlee-cn/docs/next/examples/export-entire-dataset","docId":"examples/export-entire-dataset","unlisted":false},{"type":"link","label":"Forms","href":"/crawlee-cn/docs/next/examples/forms","docId":"examples/forms","unlisted":false},{"type":"link","label":"HTTP crawler","href":"/crawlee-cn/docs/next/examples/http-crawler","docId":"examples/http-crawler","unlisted":false},{"type":"link","label":"JSDOM crawler","href":"/crawlee-cn/docs/next/examples/jsdom-crawler","docId":"examples/jsdom-crawler","unlisted":false},{"type":"link","label":"Dataset Map and Reduce methods","href":"/crawlee-cn/docs/next/examples/map-and-reduce","docId":"examples/map-and-reduce","unlisted":false},{"type":"link","label":"Playwright crawler","href":"/crawlee-cn/docs/next/examples/playwright-crawler","docId":"examples/playwright-crawler","unlisted":false},{"type":"link","label":"Using Firefox browser with Playwright crawler","href":"/crawlee-cn/docs/next/examples/playwright-crawler-firefox","docId":"examples/playwright-crawler-firefox","unlisted":false},{"type":"link","label":"Capture a screenshot using Puppeteer","href":"/crawlee-cn/docs/next/examples/capture-screenshot","docId":"examples/capture-screenshot","unlisted":false},{"type":"link","label":"Puppeteer crawler","href":"/crawlee-cn/docs/next/examples/puppeteer-crawler","docId":"examples/puppeteer-crawler","unlisted":false},{"type":"link","label":"Puppeteer recursive crawl","href":"/crawlee-cn/docs/next/examples/puppeteer-recursive-crawl","docId":"examples/puppeteer-recursive-crawl","unlisted":false},{"type":"link","label":"Skipping navigations for certain requests","href":"/crawlee-cn/docs/next/examples/skip-navigation","docId":"examples/skip-navigation","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/crawlee-cn/docs/next/examples"},{"type":"category","label":"Experiments","items":[{"type":"link","label":"Request Locking","href":"/crawlee-cn/docs/next/experiments/experiments-request-locking","docId":"experiments/experiments-request-locking","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/crawlee-cn/docs/next/experiments"},{"type":"category","label":"Upgrading","items":[{"type":"link","label":"Upgrading to v1","href":"/crawlee-cn/docs/next/upgrading/upgrading-to-v1","docId":"upgrading/upgrading-to-v1","unlisted":false},{"type":"link","label":"Upgrading to v2","href":"/crawlee-cn/docs/next/upgrading/upgrading-to-v2","docId":"upgrading/upgrading-to-v2","unlisted":false},{"type":"link","label":"Upgrading to v3","href":"/crawlee-cn/docs/next/upgrading/upgrading-to-v3","docId":"upgrading/upgrading-to-v3","unlisted":false}],"collapsed":true,"collapsible":true,"href":"/crawlee-cn/docs/next/upgrading"}]},"docs":{"deployment/apify-platform":{"id":"deployment/apify-platform","title":"Apify Platform","description":"Apify platform - large-scale and high-performance web scraping","sidebar":"docs"},"deployment/aws-browsers":{"id":"deployment/aws-browsers","title":"Browsers on AWS Lambda","description":"Running browser-enabled Crawlee crawlers in AWS Lambda is a bit complicated - but not too much. The main problem is that we have to upload not only our code and the dependencies, but also the browser binaries.","sidebar":"docs"},"deployment/aws-cheerio":{"id":"deployment/aws-cheerio","title":"Cheerio on AWS Lambda","description":"Locally, we can conveniently create a Crawlee project with npx crawlee create. In order to run this project on AWS Lambda, however, we need to do a few tweaks.","sidebar":"docs"},"deployment/gcp-browsers":{"id":"deployment/gcp-browsers","title":"Browsers in GCP Cloud Run","description":"Running full-size browsers on GCP Cloud Functions is actually a bit different from doing so on AWS Lambda - apparently, the latest runtime versions miss dependencies required to run Chromium.","sidebar":"docs"},"deployment/gcp-cheerio":{"id":"deployment/gcp-cheerio","title":"Cheerio on GCP Cloud Functions","description":"Running CheerioCrawler-based project in GCP functions is actually quite easy - you just have to make a few changes to the project code.","sidebar":"docs"},"examples/accept-user-input":{"id":"examples/accept-user-input","title":"Accept user input","description":"This example accepts and logs user input:","sidebar":"docs"},"examples/add-data-to-dataset":{"id":"examples/add-data-to-dataset","title":"Add data to dataset","description":"This example saves data to the default dataset. If the dataset doesn\'t exist, it will be created.","sidebar":"docs"},"examples/basic-crawler":{"id":"examples/basic-crawler","title":"Basic crawler","description":"This is the most bare-bones example of using Crawlee, which demonstrates some of its building blocks such as the BasicCrawler. You probably don\'t need to go this deep though, and it would be better to start with one of the full-featured crawlers","sidebar":"docs"},"examples/capture-screenshot":{"id":"examples/capture-screenshot","title":"Capture a screenshot using Puppeteer","description":"Using Puppeteer directly","sidebar":"docs"},"examples/cheerio-crawler":{"id":"examples/cheerio-crawler","title":"Cheerio crawler","description":"This example demonstrates how to use CheerioCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, parse the HTML using the Cheerio library and extract some data from it: the page title and all h1 tags.","sidebar":"docs"},"examples/crawl-all-links":{"id":"examples/crawl-all-links","title":"Crawl all links on a website","description":"This example uses the enqueueLinks() method to add new links to the RequestQueue","sidebar":"docs"},"examples/crawl-multiple-urls":{"id":"examples/crawl-multiple-urls","title":"Crawl multiple URLs","description":"This example crawls the specified list of URLs.","sidebar":"docs"},"examples/crawl-relative-links":{"id":"examples/crawl-relative-links","title":"Crawl a website with relative links","description":"When crawling a website, you may encounter different types of links present that you may want to crawl.","sidebar":"docs"},"examples/crawl-single-url":{"id":"examples/crawl-single-url","title":"Crawl a single URL","description":"This example uses the got-scraping npm package","sidebar":"docs"},"examples/crawl-sitemap":{"id":"examples/crawl-sitemap","title":"Crawl a sitemap","description":"This example downloads and crawls the URLs from a sitemap, by using the downloadListOfUrls utility method provided by the @crawlee/utils module.","sidebar":"docs"},"examples/crawl-some-links":{"id":"examples/crawl-some-links","title":"Crawl some links on a website","description":"This CheerioCrawler example uses the globs property in the enqueueLinks() method to only add links to the RequestQueue queue if they match the specified pattern.","sidebar":"docs"},"examples/crawler-plugins/playwright-puppeteer-extra":{"id":"examples/crawler-plugins/playwright-puppeteer-extra","title":"Using puppeteer-extra and playwright-extra","description":"puppeteer-extra and playwright-extra are community-built","sidebar":"docs"},"examples/export-entire-dataset":{"id":"examples/export-entire-dataset","title":"Export entire dataset to one file","description":"This Dataset example uses the exportToValue function to export the entire default dataset to a single CSV file into the default key-value store.","sidebar":"docs"},"examples/forms":{"id":"examples/forms","title":"Forms","description":"This example demonstrates how to use PuppeteerCrawler to","sidebar":"docs"},"examples/http-crawler":{"id":"examples/http-crawler","title":"HTTP crawler","description":"This example demonstrates how to use HttpCrawler to crawl a list of URLs from an external file, load each URL using a plain HTTP request, and save HTML.","sidebar":"docs"},"examples/jsdom-crawler":{"id":"examples/jsdom-crawler","title":"JSDOM crawler","description":"This example demonstrates how to use JSDOMCrawler to interact with a website using jsdom DOM implementation.","sidebar":"docs"},"examples/map-and-reduce":{"id":"examples/map-and-reduce","title":"Dataset Map and Reduce methods","description":"This example shows an easy use-case of the Dataset map","sidebar":"docs"},"examples/playwright-crawler":{"id":"examples/playwright-crawler","title":"Playwright crawler","description":"This example demonstrates how to use PlaywrightCrawler in combination with RequestQueue to recursively scrape the Hacker News website using headless Chrome / Playwright.","sidebar":"docs"},"examples/playwright-crawler-firefox":{"id":"examples/playwright-crawler-firefox","title":"Using Firefox browser with Playwright crawler","description":"This example demonstrates how to use PlaywrightCrawler with headless Firefox browser.","sidebar":"docs"},"examples/puppeteer-crawler":{"id":"examples/puppeteer-crawler","title":"Puppeteer crawler","description":"This example demonstrates how to use PuppeteerCrawler in combination","sidebar":"docs"},"examples/puppeteer-recursive-crawl":{"id":"examples/puppeteer-recursive-crawl","title":"Puppeteer recursive crawl","description":"Run the following example to perform a recursive crawl of a website using PuppeteerCrawler.","sidebar":"docs"},"examples/skip-navigation":{"id":"examples/skip-navigation","title":"Skipping navigations for certain requests","description":"While crawling a website, you may encounter certain resources you\'d like to save, but don\'t need the full power of a crawler to do so (like images delivered through a CDN).","sidebar":"docs"},"experiments/experiments-request-locking":{"id":"experiments/experiments-request-locking","title":"Request Locking","description":"Parallelize crawlers with ease using request locking","sidebar":"docs"},"guides/avoid-blocking":{"id":"guides/avoid-blocking","title":"Avoid getting blocked","description":"How to avoid getting blocked when scraping","sidebar":"docs"},"guides/cheerio-crawler-guide":{"id":"guides/cheerio-crawler-guide","title":"CheerioCrawler guide","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"guides/configuration":{"id":"guides/configuration","title":"Configuration","description":"Configuring Crawlee parameters","sidebar":"docs"},"guides/docker-images":{"id":"guides/docker-images","title":"Running in Docker","description":"Example Docker images to run your crawlers","sidebar":"docs"},"guides/got-scraping":{"id":"guides/got-scraping","title":"Got Scraping","description":"Blazing fast cURL alternative for modern web scraping","sidebar":"docs"},"guides/javascript-rendering":{"id":"guides/javascript-rendering","title":"JavaScript rendering","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"guides/jsdom-crawler-guide":{"id":"guides/jsdom-crawler-guide","title":"JSDOMCrawler guide","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"guides/motivation":{"id":"guides/motivation","title":"motivation","description":""},"guides/proxy-management":{"id":"guides/proxy-management","title":"Proxy Management","description":"Using proxies to get around those annoying IP-blocks","sidebar":"docs"},"guides/request-storage":{"id":"guides/request-storage","title":"Request Storage","description":"How to store the requests your crawler will go through","sidebar":"docs"},"guides/result-storage":{"id":"guides/result-storage","title":"Result Storage","description":"Where are you going to store all of that juicy scraped data?!","sidebar":"docs"},"guides/scaling-crawlers":{"id":"guides/scaling-crawlers","title":"Scaling our crawlers","description":"To infinity and beyond! ...within limits","sidebar":"docs"},"guides/session-management":{"id":"guides/session-management","title":"Session Management","description":"How to manage your cookies, proxy IP rotations and more","sidebar":"docs"},"guides/typescript-project":{"id":"guides/typescript-project","title":"TypeScript Projects","description":"Stricter, safer, and better development experience","sidebar":"docs"},"introduction/adding-urls":{"id":"introduction/adding-urls","title":"Adding more URLs","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/crawling":{"id":"introduction/crawling","title":"Crawling the Store","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/deployment":{"id":"introduction/deployment","title":"Running your crawler in the Cloud","description":"Deploying Crawlee projects to the Apify Platform","sidebar":"docs"},"introduction/first-crawler":{"id":"introduction/first-crawler","title":"First crawler","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/introduction":{"id":"introduction/introduction","title":"Introduction","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/real-world-project":{"id":"introduction/real-world-project","title":"Getting some real-world data","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/refactoring":{"id":"introduction/refactoring","title":"Refactoring","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/saving-data":{"id":"introduction/saving-data","title":"Saving data","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/scraping":{"id":"introduction/scraping","title":"Scraping the Store","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"introduction/setting-up":{"id":"introduction/setting-up","title":"Setting up","description":"Your first steps into the world of scraping with Crawlee","sidebar":"docs"},"quick-start/quick-start":{"id":"quick-start/quick-start","title":"Quick Start","description":"With this short tutorial you can start scraping with Crawlee in a minute or two. To learn more, read the Introduction.","sidebar":"docs"},"upgrading/upgrading-to-v1":{"id":"upgrading/upgrading-to-v1","title":"Upgrading to v1","description":"Summary","sidebar":"docs"},"upgrading/upgrading-to-v2":{"id":"upgrading/upgrading-to-v2","title":"Upgrading to v2","description":"- BREAKING: Require Node.js >=15.10.0 because HTTP2 support on lower Node.js versions is very buggy.","sidebar":"docs"},"upgrading/upgrading-to-v3":{"id":"upgrading/upgrading-to-v3","title":"Upgrading to v3","description":"This page summarizes most of the breaking changes between Crawlee (v3) and Apify SDK (v2). Crawlee is the spiritual successor to Apify SDK, so we decided to keep the versioning and release Crawlee as v3.","sidebar":"docs"}}}')}}]);