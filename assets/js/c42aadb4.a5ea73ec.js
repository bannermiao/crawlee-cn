"use strict";(self.webpackChunk=self.webpackChunk||[]).push([[8321],{2880:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>h,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>l,toc:()=>d});var r=n(5893),s=n(1151),i=n(5316);n(4959);const a={code:"import { PlaywrightCrawler } from 'crawlee';\n\nconst crawler = new PlaywrightCrawler({\n    requestHandler: async ({ page, request, enqueueLinks }) => {\n        console.log(`Processing: ${request.url}`);\n        if (request.label === 'DETAIL') {\n            const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\n            const manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n\n            const title = await page.locator('.product-meta h1').textContent();\n            const sku = await page.locator('span.product-meta__sku-number').textContent();\n\n            const priceElement = page\n                .locator('span.price')\n                .filter({\n                    hasText: '$',\n                })\n                .first();\n\n            const currentPriceString = await priceElement.textContent();\n            const rawPrice = currentPriceString.split('$')[1];\n            const price = Number(rawPrice.replaceAll(',', ''));\n\n            const inStockElement = page\n                .locator('span.product-form__inventory')\n                .filter({\n                    hasText: 'In stock',\n                })\n                .first();\n\n            const inStock = (await inStockElement.count()) > 0;\n\n            const results = {\n                url: request.url,\n                manufacturer,\n                title,\n                sku,\n                currentPrice: price,\n                availableInStock: inStock,\n            };\n\n            console.log(results);\n        } else if (request.label === 'CATEGORY') {\n            // We are now on a category page. We can use this to paginate through and enqueue all products,\n            // as well as any subsequent pages we find\n\n            await page.waitForSelector('.product-item > a');\n            await enqueueLinks({\n                selector: '.product-item > a',\n                label: 'DETAIL', // <= note the different label\n            });\n\n            // Now we need to find the \"Next\" button and enqueue the next page of results (if it exists)\n            const nextButton = await page.$('a.pagination__next');\n            if (nextButton) {\n                await enqueueLinks({\n                    selector: 'a.pagination__next',\n                    label: 'CATEGORY', // <= note the same label\n                });\n            }\n        } else {\n            // This means we're on the start page, with no label.\n            // On this page, we just want to enqueue all the category pages.\n\n            await page.waitForSelector('.collection-block-item');\n            await enqueueLinks({\n                selector: '.collection-block-item',\n                label: 'CATEGORY',\n            });\n        }\n    },\n\n    // Let's limit our crawls to make our tests shorter and safer.\n    maxRequestsPerCrawl: 50,\n});\n\nawait crawler.run(['https://warehouse-theme-metal.myshopify.com/collections']);\n",hash:"invalid-token"},o={id:"scraping",title:"Scraping the Store",sidebar_label:"Scraping",description:"Your first steps into the world of scraping with Crawlee"},c=void 0,l={id:"introduction/scraping",title:"Scraping the Store",description:"Your first steps into the world of scraping with Crawlee",source:"@site/../docs/introduction/06-scraping.mdx",sourceDirName:"introduction",slug:"/introduction/scraping",permalink:"/crawlee-cn/docs/next/introduction/scraping",draft:!1,unlisted:!1,tags:[],version:"current",sidebarPosition:6,frontMatter:{id:"scraping",title:"Scraping the Store",sidebar_label:"Scraping",description:"Your first steps into the world of scraping with Crawlee"},sidebar:"docs",previous:{title:"Crawling",permalink:"/crawlee-cn/docs/next/introduction/crawling"},next:{title:"Saving data",permalink:"/crawlee-cn/docs/next/introduction/saving-data"}},h={},d=[{value:"Scraping the URL, Manufacturer and SKU",id:"scraping-the-url-manufacturer-and-sku",level:3},{value:"Title",id:"title",level:3},{value:"SKU",id:"sku",level:3},{value:"Current price",id:"current-price",level:3},{value:"Stock available",id:"stock-available",level:3},{value:"Trying it out",id:"trying-it-out",level:2},{value:"Next lesson",id:"next-lesson",level:2}];function u(e){const t={a:"a",admonition:"admonition",code:"code",h2:"h2",h3:"h3",img:"img",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.a)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(t.p,{children:["In the ",(0,r.jsx)(t.a,{href:"./real-world-project#choosing-the-data-you-need",children:"Real-world project chapter"}),", we created a list of the information we wanted to collect about the products in the example Warehouse store. Let's review that and figure out ways to access the data."]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"URL"}),"\n",(0,r.jsx)(t.li,{children:"Manufacturer"}),"\n",(0,r.jsx)(t.li,{children:"SKU"}),"\n",(0,r.jsx)(t.li,{children:"Title"}),"\n",(0,r.jsx)(t.li,{children:"Current price"}),"\n",(0,r.jsx)(t.li,{children:"Stock available"}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"data to scrape",src:n(394).Z+"",title:"Overview of data to be scraped.",width:"1798",height:"1165"})}),"\n",(0,r.jsx)(t.h3,{id:"scraping-the-url-manufacturer-and-sku",children:"Scraping the URL, Manufacturer and SKU"}),"\n",(0,r.jsxs)(t.p,{children:["Some information is lying right there in front of us without even having to touch the product detail pages. The ",(0,r.jsx)(t.code,{children:"URL"})," we already have - the ",(0,r.jsx)(t.code,{children:"request.url"}),". And by looking at it carefully, we realize that we can also extract the manufacturer from the URL (as all product urls start with ",(0,r.jsx)(t.code,{children:"/products/<manufacturer>"}),"). We can just split the ",(0,r.jsx)(t.code,{children:"string"})," and be on our way then!"]}),"\n",(0,r.jsx)(t.admonition,{type:"info",children:(0,r.jsxs)(t.p,{children:["You can use ",(0,r.jsx)(t.code,{children:"request.loadedUrl"})," as well. Remember the difference: ",(0,r.jsx)(t.code,{children:"request.url"})," is what you enqueue, ",(0,r.jsx)(t.code,{children:"request.loadedUrl"})," is what gets processed (after possible redirects)."]})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-js",children:"// request.url = https://warehouse-theme-metal.myshopify.com/products/sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440\n\nconst urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart[0].split('-')[0]; // 'sennheiser'\n"})}),"\n",(0,r.jsx)(t.admonition,{type:"tip",children:(0,r.jsxs)(t.p,{children:["It's a matter of preference, whether to store this information separately in the resulting dataset, or not. Whoever uses the dataset can easily parse the ",(0,r.jsx)(t.code,{children:"manufacturer"})," from the ",(0,r.jsx)(t.code,{children:"URL"}),", so should you duplicate the data unnecessarily? Our opinion is that unless the increased data consumption would be too large to bear, it's better to make the dataset as rich as possible. For example, someone might want to filter by ",(0,r.jsx)(t.code,{children:"manufacturer"}),"."]})}),"\n",(0,r.jsx)(t.admonition,{type:"caution",children:(0,r.jsxs)(t.p,{children:["One thing you may notice is that the ",(0,r.jsx)(t.code,{children:"manufacturer"})," might have a ",(0,r.jsx)(t.code,{children:"-"})," in its name. If that's the case, your best bet is extracting it from the details page instead, but it's not mandatory. At the end of the day, you should always adjust and pick the best solution for your use case, and website you are crawling."]})}),"\n",(0,r.jsxs)(t.p,{children:["Now it's time to add more data to the results. Let's open one of the product detail pages, for example the ",(0,r.jsx)(t.a,{href:"https://warehouse-theme-metal.myshopify.com/products/sony-xbr-65x950g-65-class-64-5-diag-bravia-4k-hdr-ultra-hd-tv",target:"_blank",rel:"noopener",children:(0,r.jsx)(t.code,{children:"Sony XBR-950G"})})," page and use our DevTools-Fu \ud83e\udd4b to figure out how to get the title of the product."]}),"\n",(0,r.jsx)(t.h3,{id:"title",children:"Title"}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"product title",src:n(5317).Z+"",title:"Finding product title in DevTools.",width:"4106",height:"2400"})}),"\n",(0,r.jsxs)(t.p,{children:["By using the element selector tool, you can see that the title is there under an ",(0,r.jsx)(t.code,{children:"<h1>"})," tag, as titles should be. The ",(0,r.jsx)(t.code,{children:"<h1>"})," tag is enclosed in a ",(0,r.jsx)(t.code,{children:"<div>"})," with class ",(0,r.jsx)(t.code,{children:"product-meta"}),". We can leverage this to create a combined selector ",(0,r.jsx)(t.code,{children:".product-meta h1"}),". It selects any ",(0,r.jsx)(t.code,{children:"<h1>"})," element that is a child of a different element with the class ",(0,r.jsx)(t.code,{children:"product-meta"}),"."]}),"\n",(0,r.jsx)(t.admonition,{type:"tip",children:(0,r.jsxs)(t.p,{children:["Remember that you can press CTRL+F (or CMD+F on Mac) in the ",(0,r.jsx)(t.strong,{children:"Elements"})," tab of DevTools to open the search bar where you can quickly search for elements using their selectors. Always verify your scraping process and assumptions using the DevTools. It's faster than changing the crawler code all the time."]})}),"\n",(0,r.jsxs)(t.p,{children:["To get the title, we need to find it using ",(0,r.jsx)(t.code,{children:"Playwright"})," and a ",(0,r.jsx)(t.code,{children:".product-meta h1"})," locator, which selects the ",(0,r.jsx)(t.code,{children:"<h1>"})," element we're looking for, or throws, if it finds more than one. That's good. It's usually better to crash the crawler than silently return bad data."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-js",children:"const title = await page.locator('.product-meta h1').textContent();\n"})}),"\n",(0,r.jsx)(t.h3,{id:"sku",children:"SKU"}),"\n",(0,r.jsxs)(t.p,{children:["Using the DevTools, we find that the product SKU is inside a ",(0,r.jsx)(t.code,{children:"<span>"})," tag with a class ",(0,r.jsx)(t.code,{children:"product-meta__sku-number"}),". And since there's no other ",(0,r.jsx)(t.code,{children:"<span>"})," with that class on the page, we can safely use it."]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"product sku selector",src:n(4823).Z+"",title:"Finding product SKU in DevTools.",width:"4108",height:"2392"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-js",children:"const sku = await page.locator('span.product-meta__sku-number').textContent();\n"})}),"\n",(0,r.jsx)(t.h3,{id:"current-price",children:"Current price"}),"\n",(0,r.jsxs)(t.p,{children:["DevTools tell us that the ",(0,r.jsx)(t.code,{children:"currentPrice"})," can be found in a ",(0,r.jsx)(t.code,{children:"<span>"})," element tagged with the ",(0,r.jsx)(t.code,{children:"price"})," class. But it also shows us that it is nested as raw text alongside another ",(0,r.jsx)(t.code,{children:"<span>"})," element with the ",(0,r.jsx)(t.code,{children:"visually-hidden"})," class. We don't want that, so we need to filter it out, and we can use the ",(0,r.jsx)(t.code,{children:"hasText"})," helper for that."]}),"\n",(0,r.jsx)(t.p,{children:(0,r.jsx)(t.img,{alt:"product current price selector",src:n(9976).Z+"",title:"Finding product current price in DevTools.",width:"4108",height:"2396"})}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-js",children:"const priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n"})}),"\n",(0,r.jsxs)(t.p,{children:["It might look a little too complex at first glance, but let's walk through what we did. First off, we find the right part of the ",(0,r.jsx)(t.code,{children:"price"})," span (specifically the actual price) by filtering the element that has the ",(0,r.jsx)(t.code,{children:"$"})," sign in it. When we do that, we will get a string similar to ",(0,r.jsx)(t.code,{children:"Sale price$1,398.00"}),". This, by itself, is not that useful, so we extract the actual numeric part by splitting by the ",(0,r.jsx)(t.code,{children:"$"})," sign."]}),"\n",(0,r.jsxs)(t.p,{children:["Once we do that, we receive a string that represents our price, but we will be converting it to a number. We do that by replacing all the commas with nothingness (so we can parse it into a number), then we parse it into a number using ",(0,r.jsx)(t.code,{children:"Number()"}),"."]}),"\n",(0,r.jsx)(t.h3,{id:"stock-available",children:"Stock available"}),"\n",(0,r.jsxs)(t.p,{children:["We're finishing up with the ",(0,r.jsx)(t.code,{children:"availableInStock"}),". We can see there is a span with the ",(0,r.jsx)(t.code,{children:"product-form__inventory"})," class, and it contains the text ",(0,r.jsx)(t.code,{children:"In stock"}),". We can use the ",(0,r.jsx)(t.code,{children:"hasText"})," helper again to filter out the right element."]}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-js",children:"const inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n"})}),"\n",(0,r.jsxs)(t.p,{children:["For this, all we care about is whether the element exists or not, so we can use the ",(0,r.jsx)(t.code,{children:"count()"})," method to check if there are any elements that match our selector. If there are, we know that the product is in stock."]}),"\n",(0,r.jsx)(t.p,{children:"And there we have it! All the data we needed. For the sake of completeness, let's add all the properties together, and we're good to go."}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-js",children:"const urlPart = request.url.split('/').slice(-1); // ['sennheiser-mke-440-professional-stereo-shotgun-microphone-mke-440']\nconst manufacturer = urlPart.split('-')[0]; // 'sennheiser'\n\nconst title = await page.locator('.product-meta h1').textContent();\nconst sku = await page.locator('span.product-meta__sku-number').textContent();\n\nconst priceElement = page\n    .locator('span.price')\n    .filter({\n        hasText: '$',\n    })\n    .first();\n\nconst currentPriceString = await priceElement.textContent();\nconst rawPrice = currentPriceString.split('$')[1];\nconst price = Number(rawPrice.replaceAll(',', ''));\n\nconst inStockElement = await page\n    .locator('span.product-form__inventory')\n    .filter({\n        hasText: 'In stock',\n    })\n    .first();\n\nconst inStock = (await inStockElement.count()) > 0;\n"})}),"\n",(0,r.jsx)(t.h2,{id:"trying-it-out",children:"Trying it out"}),"\n",(0,r.jsxs)(t.p,{children:["We have everything we need so grab our newly created scraping logic, dump it into our original ",(0,r.jsx)(t.code,{children:"requestHandler()"})," and see the magic happen!"]}),"\n",(0,r.jsx)(i.Z,{className:"language-js",type:"playwright",children:a}),"\n",(0,r.jsx)(t.p,{children:"When you run the crawler, you will see the crawled URLs and their scraped data printed to the console. The output will look something like this:"}),"\n",(0,r.jsx)(t.pre,{children:(0,r.jsx)(t.code,{className:"language-json",children:'{\n    "url": "https://warehouse-theme-metal.myshopify.com/products/sony-str-za810es-7-2-channel-hi-res-wi-fi-network-av-receiver",\n    "manufacturer": "sony",\n    "title": "Sony STR-ZA810ES 7.2-Ch Hi-Res Wi-Fi Network A/V Receiver",\n    "sku": "SON-692802-STR-DE",\n    "currentPrice": 698,\n    "availableInStock": true\n}\n'})}),"\n",(0,r.jsx)(t.h2,{id:"next-lesson",children:"Next lesson"}),"\n",(0,r.jsx)(t.p,{children:"In the next lesson, we will show you how to save the data you scraped to the disk for further processing."})]})}function p(e={}){const{wrapper:t}={...(0,s.a)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(u,{...e})}):u(e)}},4959:(e,t,n)=>{n.d(t,{Z:()=>l});n(7294);var r=n(9960),s=n(4477),i=n(2263),a=n(5893),o=n(643).version.split("."),c=[o[0],o[1]].join(".");const l=function(e){var t=e.to,n=e.children,o=(0,s.E)();return(0,i.default)().siteConfig.presets[0][1].docs.disableVersioning||o.version===c?(0,a.jsx)(r.default,{to:"/api/"+t,children:n}):(0,a.jsx)(r.default,{to:"/api/"+("current"===o.version?"next":o.version)+"/"+t,children:n})}},5316:(e,t,n)=>{n.d(t,{Z:()=>d});var r=n(3366),s=(n(7294),n(512)),i=n(3e3),a=n(9960);const o={button:"button_YBBj",container:"container_TGAW"};var c=n(5893),l=["children","actor","hash","type"],h={playwright:"6i5QsHBMtm3hKph70",puppeteer:"7tWSD8hrYzuc9Lte7",cheerio:"kk67IcZkKSSBTslXI"};const d=function(e){var t,n=e.children,d=e.actor,u=e.hash,p=e.type,m=(0,r.Z)(e,l);if(u=null!=(t=u)?t:n.hash,!n.code)throw new Error('RunnableCodeBlock requires "code" and "hash" props\nMake sure you are importing the code block contents with the roa-loader.');if(!u)return(0,c.jsx)(i.default,Object.assign({},m,{children:n.code}));var g="https://console.apify.com/actors/"+(null!=d?d:h[null!=p?p:"playwright"])+"?runConfig="+u+"&asrc=run_on_apify";return(0,c.jsxs)("div",{className:(0,s.Z)(o.container,"runnable-code-block"),children:[(0,c.jsxs)(a.default,{href:g,className:o.button,rel:"follow",children:["Run on",(0,c.jsxs)("svg",{width:"91",height:"25",viewBox:"0 0 91 25",fill:"none",xmlns:"http://www.w3.org/2000/svg",className:"apify-logo-light alignMiddle_src-theme-Footer-index-module",children:[(0,c.jsx)("path",{d:"M3.135 2.85A3.409 3.409 0 0 0 .227 6.699l2.016 14.398 8.483-19.304-7.59 1.059Z",fill:"#97D700"}),(0,c.jsx)("path",{d:"M23.604 14.847 22.811 3.78a3.414 3.414 0 0 0-3.64-3.154c-.077 0-.153.014-.228.025l-3.274.452 7.192 16.124c.54-.67.805-1.52.743-2.379Z",fill:"#71C5E8"}),(0,c.jsx)("path",{d:"M5.336 24.595c.58.066 1.169-.02 1.706-.248l12.35-5.211L13.514 5.97 5.336 24.595Z",fill:"#FF9013"}),(0,c.jsx)("path",{d:"M33.83 5.304h3.903l5.448 14.623h-3.494l-1.022-2.994h-5.877l-1.025 2.994h-3.384L33.83 5.304Zm-.177 9.032h4.14l-2-5.994h-.086l-2.054 5.994ZM58.842 5.304h3.302v14.623h-3.302V5.304ZM64.634 5.304h10.71v2.7h-7.4v4.101h5.962v2.632h-5.963v5.186h-3.309V5.303ZM82.116 14.38l-5.498-9.076h3.748l3.428 6.016h.085l3.599-6.016H91l-5.56 9.054v5.569h-3.324v-5.548ZM51.75 5.304h-7.292v14.623h3.3v-4.634h3.993a4.995 4.995 0 1 0 0-9.99Zm-.364 7.417h-3.628V7.875h3.627a2.423 2.423 0 0 1 0 4.846Z",className:"apify-logo",fill:"#000"})]})]}),(0,c.jsx)(i.default,Object.assign({},m,{className:(0,s.Z)(o.codeBlock,"code-block",null!=m.title?"has-title":"no-title"),children:n.code}))]})}},9976:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/current-price-16b0f4b92332837111d04f632234d2c3.jpg"},394:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/scraping-practice-ed4e3a233c852ffa694b80371fed9d37.jpg"},4823:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/sku-4427a5a820183e7c74fb4beeabcf9116.jpg"},5317:(e,t,n)=>{n.d(t,{Z:()=>r});const r=n.p+"assets/images/title-8f63a08e5ecf82b5547f1fac8ffc77a7.jpg"},1151:(e,t,n)=>{n.d(t,{Z:()=>o,a:()=>a});var r=n(7294);const s={},i=r.createContext(s);function a(e){const t=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(t):{...t,...e}}),[t,e])}function o(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:t},e.children)}},643:e=>{e.exports=JSON.parse('{"name":"crawlee","version":"3.6.2","description":"The scalable web crawling and scraping library for JavaScript/Node.js. Enables development of data extraction and web automation jobs (not only) with headless Chrome and Puppeteer.","engines":{"node":">=16.0.0"},"bin":"./src/cli.ts","main":"./dist/index.js","module":"./dist/index.mjs","types":"./dist/index.d.ts","exports":{".":{"import":"./dist/index.mjs","require":"./dist/index.js","types":"./dist/index.d.ts"},"./package.json":"./package.json"},"keywords":["apify","headless","chrome","puppeteer","crawler","scraper"],"author":{"name":"Apify","email":"support@apify.com","url":"https://apify.com"},"contributors":["Jan Curn <jan@apify.com>","Marek Trunkat <marek@apify.com>","Ondra Urban <ondra@apify.com>"],"license":"Apache-2.0","repository":{"type":"git","url":"git+https://github.com/apify/crawlee"},"bugs":{"url":"https://github.com/apify/crawlee/issues"},"homepage":"https://crawlee.dev","scripts":{"build":"yarn clean && yarn compile && yarn copy","clean":"rimraf ./dist","compile":"tsc -p tsconfig.build.json && gen-esm-wrapper ./dist/index.js ./dist/index.mjs","copy":"tsx ../../scripts/copy.ts"},"publishConfig":{"access":"public"},"dependencies":{"@crawlee/basic":"3.6.2","@crawlee/browser":"3.6.2","@crawlee/browser-pool":"3.6.2","@crawlee/cheerio":"3.6.2","@crawlee/cli":"3.6.2","@crawlee/core":"3.6.2","@crawlee/http":"3.6.2","@crawlee/jsdom":"3.6.2","@crawlee/linkedom":"3.6.2","@crawlee/playwright":"3.6.2","@crawlee/puppeteer":"3.6.2","@crawlee/utils":"3.6.2","import-local":"^3.1.0","tslib":"^2.4.0"},"peerDependencies":{"playwright":"*","puppeteer":"*"},"peerDependenciesMeta":{"playwright":{"optional":true},"puppeteer":{"optional":true}}}')}}]);